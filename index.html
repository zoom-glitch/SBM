<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù…ÙƒÙÙˆÙÙŠÙ† (Ù†Ø³Ø®Ø© Ù…Ø·ÙˆØ±Ø©)</title>
  
  <!-- Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø®Ø· Ø¹Ø±Ø¨ÙŠ Ø­Ø¯ÙŠØ« -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Tajawal:wght@700&display=swap" rel="stylesheet">

  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    html, body {
      width: 100%;
      height: 100%;
      background: #111; /* Ø®Ù„ÙÙŠØ© Ø¯Ø§ÙƒÙ†Ø© ÙÙŠ Ø­Ø§Ù„Ø© ØªØ£Ø®Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ */
      font-family: 'Tajawal', sans-serif; /* ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø®Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯ */
      overflow: hidden;
    }
    video {
      position: fixed;
      top: 0; left: 0;
      width: 100%;
      height: 100%;
      object-fit: cover;
      z-index: 0; /* Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© */
    }
    #overlay {
      position: fixed;
      top: 0; left: 0;
      width: 100%; height: 100%;
      display: flex;
      justify-content: center;
      align-items: center;
      padding: 20px;
      z-index: 10;
      cursor: pointer; /* Ø¥Ø¸Ù‡Ø§Ø± Ø£Ù† Ø§Ù„Ø´Ø§Ø´Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù†Ù‚Ø± */
    }
    #objects {
      background: rgba(0, 0, 0, 0.75);
      color: #00ff7f; /* Ù„ÙˆÙ† Ø£Ø®Ø¶Ø± Ø³Ø§Ø·Ø¹ (SpringGreen) */
      padding: 30px 40px;
      border-radius: 20px;
      border: 2px solid rgba(0, 255, 127, 0.5);
      font-size: 38px;
      font-weight: bold;
      text-align: center;
      max-width: 95%;
      line-height: 1.6;
      text-shadow: 0 0 10px rgba(0, 0, 0, 0.7); /* Ø¸Ù„ Ù„Ù„Ù†Øµ Ù„Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© */
      box-shadow: 0 4px 30px rgba(0, 0, 0, 0.4);
      backdrop-filter: blur(5px);
      -webkit-backdrop-filter: blur(5px);
      transition: all 0.3s ease;
    }
    #objects.listening {
        background-color: rgba(0, 50, 100, 0.8);
        border-color: rgba(0, 150, 255, 0.8);
    }
  </style>
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <div id="overlay">
      <div id="objects">ğŸ“¦ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚...</div>
  </div>

  <!-- Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.14.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <script src="https://cdn.jsdelivr.net/npm/tesseract.js@2.1.5/dist/tesseract.min.js"></script>

  <script>
    const video = document.getElementById("video");
    const objectDiv = document.getElementById("objects");
    const overlay = document.getElementById("overlay");
    let model;
    let isProcessing = false;

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    let recognition;

    if (SpeechRecognition) {
        recognition = new SpeechRecognition();
        recognition.lang = 'ar-EG';
        recognition.continuous = false;
        recognition.interimResults = false;
    } else {
        objectDiv.innerText = "âŒ Ø§Ù„Ù…ØªØµÙØ­ Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØµÙˆØªÙŠØ©";
    }

    async function startCamera() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "environment" }, audio: false });
        video.srcObject = stream;
      } catch (e) { objectDiv.innerText = "âŒ ÙØ´Ù„ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§"; console.error("Camera error:", e); }
    }

    async function loadModel() {
      try {
        objectDiv.innerText = "ğŸ“¦ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ...";
        model = await cocoSsd.load();
        objectDiv.innerText = "âœ… Ø¬Ø§Ù‡Ø²!\nØ§Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø© Ø«Ù… ØªØ­Ø¯Ø«.";
        speak("Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¬Ø§Ù‡Ø². Ø§Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø© Ù„Ø¥Ø¹Ø·Ø§Ø¡ Ø£Ù…Ø± ØµÙˆØªÙŠ.");
      } catch (e) { objectDiv.innerText = "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"; console.error("Model load error:", e); }
    }

    function getArabicVoice() {
      const voices = speechSynthesis.getVoices();
      return voices.find(v => v.lang.includes("ar")) || null;
    }

    function speak(text) {
      if (!text || text.trim() === "") return;
      const msg = new SpeechSynthesisUtterance(text);
      msg.lang = 'ar-EG';
      msg.voice = getArabicVoice();
      msg.rate = 0.95;
      speechSynthesis.cancel();
      speechSynthesis.speak(msg);
    }
    
    function playBeep(start = true) {
        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const oscillator = audioContext.createOscillator();
        oscillator.type = 'sine';
        oscillator.frequency.setValueAtTime(start ? 880 : 520, audioContext.currentTime);
        oscillator.connect(audioContext.destination);
        oscillator.start();
        oscillator.stop(audioContext.currentTime + 0.1);
    }

    function joinWithWa(list) {
      if (list.length === 0) return "";
      if (list.length === 1) return list[0];
      if (list.length === 2) return list[0] + " Ùˆ" + list[1];
      return list.slice(0, -1).join("ØŒ ") + "ØŒ Ùˆ" + list[list.length - 1];
    }

    async function translate(text) {
      try {
        const res = await fetch(`https://api.mymemory.translated.net/get?q=${text}&langpair=en|ar`);
        const data = await res.json();
        return data.responseData.translatedText.toLowerCase() || text;
      } catch { return text; }
    }

    // --- ÙˆØ¸ÙŠÙØ© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙ‚Ø· ---
    async function runObjectDetection() {
        objectDiv.innerText = "ğŸ¤” Ù…Ø§Ø°Ø§ Ø£Ø±Ù‰ Ø£Ù…Ø§Ù…ÙŠØŸ...";
        const predictions = await model.detect(video);
        const labels = [...new Set(predictions.map(p => p.class))];
        
        if (labels.length === 0) {
            return "Ù„Ø§ Ø£Ø±Ù‰ Ø´ÙŠØ¦Ù‹Ø§ ÙˆØ§Ø¶Ø­Ù‹Ø§.";
        }
        const translatedLabels = await Promise.all(labels.map(translate));
        return "Ø£Ø±Ù‰ Ø£Ù…Ø§Ù…Ùƒ " + joinWithWa(translatedLabels) + ".";
    }

    // --- ÙˆØ¸ÙŠÙØ© Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†ØµÙˆØµ ÙÙ‚Ø· ---
    async function runTextRecognition() {
        objectDiv.innerText = "ğŸ“– Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†Øµ...";
        const { data: { text } } = await Tesseract.recognize(video, 'eng+ara');
        const cleanText = text.trim().replace(/\n/g, ' ').replace(/\s+/g, ' ').trim();
        
        return cleanText ? "Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙƒØªÙˆØ¨ Ù‡Ùˆ: " + cleanText : "Ù„Ù… Ø£Ø¬Ø¯ Ø£ÙŠ Ù†Øµ Ù„Ø£Ù‚Ø±Ø£Ù‡.";
    }

    if (recognition) {
        recognition.onresult = async (event) => {
            const command = event.results[0][0].transcript.trim().toLowerCase();
            console.log('ØªÙ… Ø³Ù…Ø§Ø¹ Ø§Ù„Ø£Ù…Ø±:', command);
            objectDiv.classList.remove('listening');
            playBeep(false);

            let resultMessage = "";
            try {
                 if (command.includes("Ù…Ø§Ø°Ø§ Ø£Ù…Ø§Ù…ÙŠ") || command.includes("Ù…Ø§ Ø§Ù„Ø°ÙŠ Ø£Ù…Ø§Ù…ÙŠ") || command.includes("Ø´Ø§ÙŠÙ Ø§ÙŠÙ‡")) {
                    resultMessage = await runObjectDetection();
                } else if (command.includes("Ø§Ù‚Ø±Ø£") || command.includes("Ø§Ù‚Ø±Ø§") || command.includes("Ù…ÙƒØªÙˆØ¨ Ø§ÙŠÙ‡")) {
                    resultMessage = await runTextRecognition();
                } else {
                    resultMessage = "Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ø£Ù…Ø±. Ù‚Ù„ 'Ù…Ø§Ø°Ø§ Ø£Ù…Ø§Ù…ÙŠ' Ø£Ùˆ 'Ø§Ù‚Ø±Ø£ Ø§Ù„Ù†Øµ'.";
                }
            } catch(err) {
                console.error("Processing error:", err);
                resultMessage = "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙ†ÙÙŠØ° Ø·Ù„Ø¨Ùƒ.";
            }

            objectDiv.innerText = resultMessage;
            speak(resultMessage);
            isProcessing = false;
        };

        recognition.onerror = (event) => {
            console.error("Speech recognition error", event.error);
            objectDiv.innerText = "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø§ÙŠÙƒØ±ÙˆÙÙˆÙ†.\nØ§Ù†Ù‚Ø± Ù„Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø¬Ø¯Ø¯Ø§Ù‹.";
            objectDiv.classList.remove('listening');
            isProcessing = false;
        };
    }

    function listenForCommand() {
        if (isProcessing || !recognition) return;
        
        isProcessing = true;
        playBeep(true);
        objectDiv.innerText = "ğŸ¤ Ø§Ø³ØªÙ…Ø¹ Ø§Ù„Ø¢Ù†...";
        objectDiv.classList.add('listening');
        
        try {
            recognition.start();
        } catch (e) {
            isProcessing = false;
            objectDiv.classList.remove('listening');
            console.error("Error starting recognition:", e);
        }
    }

    async function init() {
      await startCamera();
      await loadModel();
      overlay.addEventListener("click", listenForCommand);
    }
    
    window.addEventListener("load", () => {
        setTimeout(() => speechSynthesis.getVoices(), 200);
    });

    init();
  </script>
</body>
</html>
