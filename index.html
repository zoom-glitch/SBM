<!DOCTYPE html>
<html lang="ar">
<head>
  <meta charset="UTF-8" />
  <title>Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙƒÙÙˆÙÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ</title>
  <style>
    * { margin: 0; padding: 0; }
    html, body {
      width: 100%;
      height: 100%;
      background: black;
      font-family: 'Arial', sans-serif;
      overflow: hidden;
    }
    video {
      position: fixed;
      top: 0; left: 0;
      width: 100vw;
      height: 100vh;
      object-fit: cover;
      z-index: 0;
      background: black;
    }
    #objects {
      position: fixed;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      background: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 30px 40px;
      border-radius: 20px;
      font-size: 48px;
      font-weight: bold;
      text-align: center;
      max-width: 90%;
      line-height: 1.5;
      z-index: 10;
    }
  </style>
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <div id="objects">ğŸ“¦ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚...</div>

  <!-- Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.14.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <script src="https://cdn.jsdelivr.net/npm/tesseract.js@2.1.5/dist/tesseract.min.js"></script>

  <script>
    const video = document.getElementById("video");
    const objectDiv = document.getElementById("objects");
    let model;
    let lastSaid = "";

    async function startCamera() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { facingMode: "environment" },
          audio: false
        });
        video.srcObject = stream;
      } catch (e) {
        objectDiv.innerText = "âŒ ÙØ´Ù„ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§";
        console.error("Camera error:", e);
      }
    }

    async function loadModel() {
      try {
        objectDiv.innerText = "ğŸ“¦ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ...";
        model = await cocoSsd.load();
        objectDiv.innerText = "âœ… Ø¬Ø§Ù‡Ø²! ÙˆØ¬Ù‡ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ù„Ø£ÙŠ Ø´ÙŠØ¡.";
      } catch (e) {
        objectDiv.innerText = "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬";
        console.error("Model load error:", e);
      }
    }

    function getArabicVoice() {
      const voices = speechSynthesis.getVoices();
      return voices.find(v => v.lang.includes("ar") && v.name.includes("Google")) ||
             voices.find(v => v.lang.includes("ar")) || null;
    }

    function speak(text) {
      const msg = new SpeechSynthesisUtterance(text);
      msg.lang = 'ar-EG';
      msg.voice = getArabicVoice();
      msg.rate = 0.85;
      speechSynthesis.cancel();
      speechSynthesis.speak(msg);
    }

    function joinWithWa(list) {
      if (list.length === 1) return list[0];
      if (list.length === 2) return list[0] + " Ùˆ" + list[1];
      return list.slice(0, -1).join("ØŒ ") + " Ùˆ" + list[list.length - 1];
    }

    async function translate(text) {
      try {
        const res = await fetch(`https://api.mymemory.translated.net/get?q=${text}&langpair=en|ar`);
        const data = await res.json();
        return data.responseData.translatedText || text;
      } catch {
        return text;
      }
    }

    async function detectObjectsAndText() {
      try {
        const predictions = await model.detect(video);
        const labels = [...new Set(predictions.map(p => p.class))];
        const translatedLabels = await Promise.all(labels.map(translate));

        const canvas = document.createElement("canvas");
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        const ctx = canvas.getContext("2d");
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        const { data: { text } } = await Tesseract.recognize(canvas, 'eng');
        const cleanText = text.trim().split("\n").filter(t => t.length > 2).join("ØŒ ");
        let fullText = "";

        if (translatedLabels.length > 0) {
          fullText = "Ù‚Ø¯Ø§Ù…Ùƒ " + joinWithWa(translatedLabels);
        }
        if (cleanText) {
          fullText += (fullText ? "ØŒ " : "") + "ÙˆÙ…ÙƒØªÙˆØ¨: " + cleanText;
        }

        if (!fullText) {
          fullText = "Ù…Ø´ Ø´Ø§ÙŠÙ Ø­Ø§Ø¬Ø© ÙˆØ§Ø¶Ø­Ø©";
        }

        objectDiv.innerText = fullText;
        if (fullText !== lastSaid) {
          lastSaid = fullText;
          speak(fullText);
        }
      } catch (err) {
        console.error("Detection error:", err);
        objectDiv.innerText = "âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø£Ùˆ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬";
      }
    }

    async function init() {
      await startCamera();
      await loadModel();
      if (model) {
        setInterval(detectObjectsAndText, 3000); // ÙƒÙ„ 3 Ø«ÙˆØ§Ù†ÙŠ
      }
    }

    window.addEventListener("click", () => {
      speechSynthesis.getVoices();
    });

    init();
  </script>
</body>
</html>
