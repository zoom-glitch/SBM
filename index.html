<!DOCTYPE html>
<html lang="ar">
<head>
  <meta charset="UTF-8" />
  <title>Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙƒÙÙˆÙÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØµÙˆØªÙŠØ©</title>
  <style>
    * { margin: 0; padding: 0; }
    html, body {
      width: 100%;
      height: 100%;
      background: black;
      font-family: 'Arial', sans-serif;
      overflow: hidden;
    }
    video {
      position: fixed;
      top: 0; left: 0;
      width: 100vw;
      height: 100vh;
      object-fit: cover;
      z-index: 0;
      background: black;
    }
    #objects {
      position: fixed;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      background: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 30px 40px;
      border-radius: 20px;
      font-size: 48px;
      font-weight: bold;
      text-align: center;
      max-width: 90%;
      line-height: 1.5;
      z-index: 10;
    }
  </style>
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <div id="objects">ğŸ“¦ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚...</div>

  <!-- Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.14.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <script src="https://cdn.jsdelivr.net/npm/tesseract.js@2.1.5/dist/tesseract.min.js"></script>

  <script>
    const video = document.getElementById("video");
    const objectDiv = document.getElementById("objects");
    let model;

    // --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª ---
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    let recognition;

    // Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…ØªØµÙØ­ ÙŠØ¯Ø¹Ù… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª
    if (SpeechRecognition) {
        recognition = new SpeechRecognition();
        recognition.lang = 'ar-EG'; // Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Ù…ØµØ±)
        recognition.continuous = true; // Ø§Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹
        recognition.interimResults = false;
    } else {
        objectDiv.innerText = "âŒ Ø§Ù„Ù…ØªØµÙØ­ Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØµÙˆØªÙŠØ©";
        console.error("Speech Recognition not supported.");
    }

    async function startCamera() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { facingMode: "environment" },
          audio: false
        });
        video.srcObject = stream;
      } catch (e) {
        objectDiv.innerText = "âŒ ÙØ´Ù„ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§";
        console.error("Camera error:", e);
      }
    }

    async function loadModel() {
      try {
        objectDiv.innerText = "ğŸ“¦ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ...";
        model = await cocoSsd.load();
        objectDiv.innerText = "âœ… Ø¬Ø§Ù‡Ø²! Ù‚Ù„ 'Ù…Ø§ Ø§Ù„Ø°ÙŠ Ø£Ù…Ø§Ù…ÙŠ' Ø£Ùˆ 'Ø§Ù‚Ø±Ø£ Ø§Ù„Ù†Øµ'.";
      } catch (e) {
        objectDiv.innerText = "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬";
        console.error("Model load error:", e);
      }
    }

    function getArabicVoice() {
      const voices = speechSynthesis.getVoices();
      return voices.find(v => v.lang.includes("ar") && v.name.includes("Google")) ||
             voices.find(v => v.lang.includes("ar")) || null;
    }

    function speak(text) {
      // Ù„Ø§ ØªÙ†Ø·Ù‚ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù†Øµ
      if (!text || text.trim() === "") return;
      const msg = new SpeechSynthesisUtterance(text);
      msg.lang = 'ar-EG';
      msg.voice = getArabicVoice();
      msg.rate = 0.9;
      speechSynthesis.cancel(); // Ø¥ÙŠÙ‚Ø§Ù Ø£ÙŠ ÙƒÙ„Ø§Ù… Ø­Ø§Ù„ÙŠ
      speechSynthesis.speak(msg);
    }

    function joinWithWa(list) {
      if (list.length === 0) return "";
      if (list.length === 1) return list[0];
      if (list.length === 2) return list[0] + " Ùˆ" + list[1];
      return list.slice(0, -1).join("ØŒ ") + " Ùˆ" + list[list.length - 1];
    }

    async function translate(text) {
      try {
        const res = await fetch(`https://api.mymemory.translated.net/get?q=${text}&langpair=en|ar`);
        const data = await res.json();
        return data.responseData.translatedText || text;
      } catch {
        return text;
      }
    }

    // --- Ø¯Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙ‚Ø· ---
    async function runObjectDetection() {
        if (!model) return;
        objectDiv.innerText = "ğŸ¤” Ø£ÙÙƒØ±... Ù…Ø§ Ø§Ù„Ø°ÙŠ Ø£Ø±Ø§Ù‡ØŸ";
        try {
            const predictions = await model.detect(video);
            const labels = [...new Set(predictions.map(p => p.class))];
            
            if (labels.length === 0) {
                const message = "Ù„Ø§ Ø£Ø±Ù‰ Ø´ÙŠØ¦Ù‹Ø§ ÙˆØ§Ø¶Ø­Ù‹Ø§.";
                objectDiv.innerText = message;
                speak(message);
                return;
            }

            const translatedLabels = await Promise.all(labels.map(translate));
            const message = "Ø£Ø±Ù‰ Ø£Ù…Ø§Ù…Ùƒ " + joinWithWa(translatedLabels);
            objectDiv.innerText = message;
            speak(message);

        } catch (err) {
            console.error("Object detection error:", err);
            objectDiv.innerText = "âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡.";
        }
    }

    // --- Ø¯Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†ØµÙˆØµ ÙÙ‚Ø· ---
    async function runTextRecognition() {
        objectDiv.innerText = "ğŸ“– Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†Øµ...";
        try {
            const canvas = document.createElement("canvas");
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            const ctx = canvas.getContext("2d");
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

            const { data: { text } } = await Tesseract.recognize(canvas, 'eng+ara'); // ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            const cleanText = text.trim().split("\n").filter(t => t.length > 2).join(" ");
            
            let message = "";
            if (cleanText) {
                message = "Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙƒØªÙˆØ¨ Ù‡Ùˆ: " + cleanText;
            } else {
                message = "Ù„Ù… Ø£Ø¬Ø¯ Ø£ÙŠ Ù†Øµ Ù„Ø£Ù‚Ø±Ø£Ù‡.";
            }
            objectDiv.innerText = message;
            speak(message);

        } catch (err) {
            console.error("Text recognition error:", err);
            objectDiv.innerText = "âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†Øµ.";
        }
    }
    
    // --- Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØµÙˆØªÙŠØ© ---
    if (recognition) {
        recognition.onresult = (event) => {
            const last = event.results.length - 1;
            const command = event.results[last][0].transcript.trim().toLowerCase();
            
            console.log('ØªÙ… Ø³Ù…Ø§Ø¹ Ø§Ù„Ø£Ù…Ø±:', command);
            
            if (command.includes("Ù…Ø§ Ø§Ù„Ø°ÙŠ Ø£Ù…Ø§Ù…ÙŠ") || command.includes("Ù…Ø§Ø°Ø§ Ø§Ù…Ø§Ù…ÙŠ")) {
                runObjectDetection();
            } else if (command.includes("Ø§Ù‚Ø±Ø£") || command.includes("Ø§Ù‚Ø±Ø§")) {
                runTextRecognition();
            }
        };
        
        // Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ø°Ø§ ØªÙˆÙ‚Ù
        recognition.onend = () => {
            console.log("Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹...");
            recognition.start();
        };
    }

    async function init() {
      await startCamera();
      await loadModel();
      // Ø¥Ø°Ø§ ØªÙ… ØªØ­Ù…ÙŠÙ„ ÙƒÙ„ Ø´ÙŠØ¡ Ø¨Ù†Ø¬Ø§Ø­ØŒ Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù„Ø£ÙˆØ§Ù…Ø±
      if (model && recognition) {
        try {
          recognition.start();
          console.log("ğŸ¤ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØµÙˆØªÙŠØ© Ø¨Ø¯Ø£.");
        } catch(e) {
          console.error("Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ØŒ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø¨Ø¯Ø£ Ø¨Ø§Ù„ÙØ¹Ù„", e);
        }
      }
    }
    
    // ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„Ù†Ù‚Ø± Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø© Ù„ØªÙØ¹ÙŠÙ„ Ø§Ù„ØµÙˆØª ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ù…ØªØµÙØ­Ø§Øª
    window.addEventListener("click", () => {
      speechSynthesis.getVoices(); 
    });

    init();
  </script>
</body>
</html>
